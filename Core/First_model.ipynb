{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1107, 91088)\n",
      "  (0, 0)\t2.0\n",
      "  (1, 0)\t2.0\n",
      "  (2, 0)\t2.0\n",
      "  (3, 0)\t2.0\n",
      "  (4, 0)\t2.0\n",
      "  (5, 0)\t2.0\n",
      "  (6, 0)\t2.0\n",
      "  (7, 0)\t2.0\n",
      "  (8, 0)\t2.0\n",
      "  (9, 0)\t2.0\n",
      "  (10, 0)\t2.0\n",
      "  (11, 0)\t2.0\n",
      "  (12, 0)\t2.0\n",
      "  (13, 0)\t2.0\n",
      "  (14, 0)\t2.0\n",
      "  (15, 0)\t1.0\n",
      "  (16, 0)\t2.0\n",
      "  (17, 0)\t2.0\n",
      "  (18, 0)\t2.0\n",
      "  (20, 0)\t2.0\n",
      "  (21, 0)\t2.0\n",
      "  (22, 0)\t2.0\n",
      "  (23, 0)\t2.0\n",
      "  (24, 0)\t2.0\n",
      "  (25, 0)\t2.0\n",
      "  :\t:\n",
      "  (1082, 0)\t2.0\n",
      "  (1083, 0)\t2.0\n",
      "  (1084, 0)\t2.0\n",
      "  (1085, 0)\t2.0\n",
      "  (1086, 0)\t2.0\n",
      "  (1087, 0)\t2.0\n",
      "  (1088, 0)\t2.0\n",
      "  (1089, 0)\t2.0\n",
      "  (1090, 0)\t2.0\n",
      "  (1091, 0)\t2.0\n",
      "  (1092, 0)\t2.0\n",
      "  (1093, 0)\t2.0\n",
      "  (1094, 0)\t2.0\n",
      "  (1095, 0)\t2.0\n",
      "  (1096, 0)\t2.0\n",
      "  (1097, 0)\t1.0\n",
      "  (1098, 0)\t2.0\n",
      "  (1099, 0)\t2.0\n",
      "  (1100, 0)\t2.0\n",
      "  (1101, 0)\t2.0\n",
      "  (1102, 0)\t2.0\n",
      "  (1103, 0)\t2.0\n",
      "  (1104, 0)\t2.0\n",
      "  (1105, 0)\t2.0\n",
      "  (1106, 0)\t2.0\n"
     ]
    }
   ],
   "source": [
    "# %load load.py\n",
    "import os\n",
    "import json\n",
    "from business import Business\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import modeldata as md\n",
    "import mlmodels as mlm\n",
    "import pickle\n",
    "import random as rd\n",
    "\n",
    "'''\n",
    "Takes as input a file with a business info on each line and splits it into separate files for each state\n",
    "'''\n",
    "def split_business_by_state(infile, outfile=None):\n",
    "    if outfile is None:\n",
    "        outfile = infile\n",
    "    outfiles = {}\n",
    "    with open(infile) as f:\n",
    "        for line in iter(f):\n",
    "            jline = json.loads(line)\n",
    "            state = jline['state']\n",
    "            if state not in outfiles:\n",
    "                print ('adding new state {}'.format(state))\n",
    "                o = outfile + '_' + state\n",
    "                outfiles[state] = open(o, 'w')\n",
    "            outfiles[state].write(line)\n",
    "    for v in iter(outfiles.values()):\n",
    "        v.close()\n",
    "\n",
    "\n",
    "'''\n",
    "Takes a file with a set of businesses and reviews, save a set of reviews that only relevant to the businesses\n",
    "'''\n",
    "def get_reviews_for_businesses(businesses, reviews, outfile=None):\n",
    "    if outfile is None:\n",
    "        outfile = businesses + '_reviews'\n",
    "    b_ids = []\n",
    "    with open(businesses) as f:\n",
    "        for line in iter(f):\n",
    "            jline = json.loads(line)\n",
    "            b_ids.append(jline['business_id'])\n",
    "        b_set = set(b_ids)\n",
    "\n",
    "    with open(reviews) as f:\n",
    "        with open(outfile, 'w') as o:\n",
    "            for line in iter(f):\n",
    "                jline = json.loads(line)\n",
    "                b_id = jline['business_id']\n",
    "                if b_id in b_set:\n",
    "                    o.write(line)\n",
    "\n",
    "'''\n",
    "Based on a business file, output a file with all possible attributes and values\n",
    "'''\n",
    "def get_attributes(business_file, outfile=None):\n",
    "    if outfile is None:\n",
    "        outfile = business_file + '_attributes'\n",
    "    attr_hash = {}\n",
    "    with open(business_file) as f:\n",
    "        for line in iter(f):\n",
    "            jline = json.loads(line)\n",
    "            attributes = jline['attributes']\n",
    "            for k, v in iter(attributes.items()):\n",
    "                if k not in attr_hash:\n",
    "                    attr_hash[k] = []\n",
    "                values = []\n",
    "                if not isinstance(v, dict):\n",
    "                    values.append(v)\n",
    "                else:\n",
    "                    for val in iter(v.keys()):\n",
    "                        values.append(val)\n",
    "                for value in values:\n",
    "                    if value not in attr_hash[k]:\n",
    "                        attr_hash[k].append(value)\n",
    "    with open(outfile, 'w') as o:\n",
    "        for k, v in iter(attr_hash.items()):\n",
    "            o.write(k + str(v) + '\\n')\n",
    "\n",
    "'''\n",
    "Reads a business file and get only restaurants\n",
    "'''\n",
    "def get_restaurants(business_file, outfile=None):\n",
    "    if outfile is None:\n",
    "        outfile = business_file + '_restaurants'\n",
    "    with open(outfile, 'w') as o:\n",
    "        with open(business_file, 'r') as f:\n",
    "            for line in iter(f):\n",
    "                jline = json.loads(line)\n",
    "                if 'Restaurants' in jline['categories']:\n",
    "                    o.write(line)\n",
    "'''\n",
    "number_set: 1-?\n",
    "train_weight: 1-9.\n",
    "'''\n",
    "def split_data(businessfile, reviewfile, number_sets, train_weight):\n",
    "    all_sets = {}\n",
    "    for i  in range(1, number_sets+1):\n",
    "        all_sets[\"train\" + str(i)] = {}\n",
    "        all_sets[\"test\" + str(i)] = {}\n",
    "\n",
    "    with open(businessfile) as rst:\n",
    "        for line in rst:\n",
    "            jline = json.loads(line)\n",
    "            b_id = jline['business_id']\n",
    "            set_num = rd.randint(1, number_sets)\n",
    "            isTrain = rd.randint(1, 10)\n",
    "            if isTrain > train_weight:\n",
    "                dict_type = \"test\"\n",
    "            else:\n",
    "                dict_type = \"train\"\n",
    "            dict_temp = {}\n",
    "            if (dict_type + str(set_num)) in all_sets.keys():\n",
    "                dict_temp = all_sets[dict_type + str(set_num)]\n",
    "            dict_temp[b_id] = Business(jline)\n",
    "            all_sets[dict_type + str(set_num)]  = dict_temp\n",
    "\n",
    "    all_keys = all_sets.keys()\n",
    "    with open(reviewfile) as rvw:\n",
    "        for line in rvw:\n",
    "            jline = json.loads(line)\n",
    "            b_id = jline['business_id']\n",
    "            for i_dict in all_keys:\n",
    "                if b_id in all_sets[i_dict].keys():\n",
    "                    dict_temp = all_sets[i_dict]\n",
    "                    dict_temp[b_id].add_review(jline)\n",
    "                    all_sets[i_dict]  = dict_temp\n",
    "    return all_sets\n",
    "if __name__ == '__main__':\n",
    "    data_dir = '../data/'\n",
    "    parsed_dir = data_dir + 'parsed/'\n",
    "    raw_dir = data_dir + 'yelp_data/'\n",
    "    '''\n",
    "    some setting up\n",
    "    '''\n",
    "    #raw_reviews = raw_dir + 'yelp_academic_dataset_review.json'\n",
    "    #business_data = raw_dir + 'yelp_academic_dataset_business.json'\n",
    "    #split_business_by_state(business_data, outfile=parsed_dir + 'businesses')\n",
    "    #get_restaurants('../data/parsed/businesses_WI')\n",
    "    #get_reviews_for_businesses(parsed_dir + 'businesses_WI_restaurants', raw_reviews)\n",
    "    #get_attributes('../data/yelp_data/yelp_academic_dataset_business.json', '../data/parsed/attributes')\n",
    "    '''\n",
    "    Creates a bag of words representation based on the WI restaurants and reviews\n",
    "    '''\n",
    "    \n",
    "    business_file = 'yelp_academic_dataset_business.json'\n",
    "    review_file = 'yelp_academic_dataset_review.json'\n",
    "    #bag_of_words = md.create_bag_of_wods(raw_dir + business_file,\n",
    "    #                                     raw_dir + review_file)\n",
    "\n",
    "\n",
    "    #bag_of_ngrams = md.create_bag_of_ngrams(parsed_dir + 'WI_test_restaurants',\n",
    "    #parsed_dir + 'WI_test_reviews')\n",
    "\n",
    "\n",
    "    #bag_of_ngrams.make_tfidf_matrix()\n",
    "    #print(bag_of_ngrams.datamatrix)\n",
    "    #print(bag_of_ngrams.datamatrix.shape)\n",
    "\n",
    "    #print(bag_of_ngrams.labels)\n",
    "    #print(bag_of_ngrams.labels.shape)\n",
    "    #exit()\n",
    "    # get_reviews_for_state('../data/parsed/businesses_TX', '../data/yelp_data/yelp_academic_dataset_review.json')\n",
    "    #all_sets = split_data(parsed_dir + 'businesses_WI_restaurants', parsed_dir + 'businesses_WI_restaurants_reviews', 2, 5)\n",
    "    #bag_of_words = md.create_bag_of_words(ba_aggr=all_sets['train1'], attribute=\"Price Range\")\n",
    "    bag_of_words = md.create_bag_of_words(parsed_dir + 'businesses_WI_restaurants',\n",
    "                                           parsed_dir + 'businesses_WI_restaurants_reviews')\n",
    "    bag_of_words.make_sparse_datamtrix()\n",
    "    bag_of_words.make_tfidf_matrix()\n",
    "    #print(len(bag_of_words.datamatrix.data))\n",
    "    print(bag_of_words.datamatrix.shape)\n",
    "    print(bag_of_words.labels[:, 0])\n",
    "\n",
    "    #json_data = open(parsed_dir + 'businesses_WI_restaurants').read()\n",
    "    #data = json.load(json_data)\n",
    " # get_reviews_for_state('../data/parsed/businesses_TX', '../data/yelp_data/yelp_academic_dataset_review.json')\n",
    " \n",
    "    '''\n",
    "    Used this for the development of the One Vs Rest model. Want to confer on architecture of program before I move things around\n",
    "    Figured it'd either be like this or maybe Class setup that calls the method and prints out internally.\n",
    "    '''\n",
    "    #print(bag_of_words.labels[0:10])\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(bag_of_words.datamatrix, bag_of_words.labels, test_size=0.3)\n",
    "    #one_v_rest = mlm.one_vs_rest(X_train,y_train)\n",
    "    #first10Predict = one_v_rest.predict(bag_of_words.datamatrix[0:10])\n",
    "    #print (first10Predict)\n",
    "    #print(one_v_rest.score(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load mlmodels.py\n",
    "import nltk\n",
    "import business\n",
    "from business import Business\n",
    "import json\n",
    "#import xgboost\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "def one_vs_rest(x_train,y_train):\n",
    "    one_v_rest = OneVsRestClassifier(LinearSVC(random_state=0)).fit(x_train, y_train)\n",
    "    return one_v_rest\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr_err_rate \t test_err rate\n",
      "6.07 \t 7.81\n",
      "not enough data\n",
      "not enough data\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X_train, X_test, y_train, y_test = train_test_split(bag_of_words.datamatrix, bag_of_words.labels, test_size=0.3)\n",
    "\n",
    "print(\"tr_err_rate \\t test_err rate\")\n",
    "for c in range(0, y_train.shape[1]): \n",
    "    y_tr = []\n",
    "    y_te = []\n",
    "    ct_data = 0\n",
    "    \n",
    "    # get training data\n",
    "    for r_tr in range(0, y_train.shape[0]):\n",
    "        y_lb_tr = y_train[r_tr,c]\n",
    "        if y_lb_tr > 0:\n",
    "            ct_data = ct_data + 1\n",
    "        y_tr.append(y_lb_tr)    \n",
    "            \n",
    "            \n",
    "    # get test data        \n",
    "    for r_te in range(0, y_test.shape[0]):\n",
    "        y_te.append(y_test[r_te,c])\n",
    "        \n",
    "    # check if valid training data is more than 70%  \n",
    "    if ct_data/y_train.shape[0] < 0.7:\n",
    "        print(\"not enough data\")\n",
    "    else:\n",
    "        one_v_rest = OneVsRestClassifier(LinearSVC(random_state=0)).fit(X_train, y_tr)\n",
    "        print(\"%.2f\" % (100*(1-one_v_rest.score(X_train, y_tr))) + \" \\t \" + \"%.2f\" % (100*(1-one_v_rest.score(X_test, y_te))))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
